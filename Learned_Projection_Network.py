{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Constants for energy dynamics and universal expansion\n",
        "ETA = 0.52\n",
        "BETA = 0.31  # Updated beta value\n",
        "LAMBDA = 0.867\n",
        "GAMMA = 0.4497  # Corrected gamma constant for energy damping\n",
        "CMB_NOISE = 0.01\n",
        "EXPANSION_RATE = 0.02\n",
        "\n",
        "# Starting ratios based on Big Bang dynamics\n",
        "QUANTUM_RATIO = 0.68  # Approximation of quantum effects\n",
        "GRAVITATIONAL_RATIO = 0.27  # Approximation of gravitational influence\n",
        "COMBINED_RATIO = 0.05  # Combined emergent energy effects\n",
        "\n",
        "class BiMMOEQDT_UniversalExpansion(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, noise_std=0.05):\n",
        "        super(BiMMOEQDT_UniversalExpansion, self).__init__()\n",
        "        self.noise_std = noise_std\n",
        "\n",
        "        # Quantum Tunneling Layer\n",
        "        self.quantum_layer = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # Gravitational Funneling Layer\n",
        "        self.gravitational_layer = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # Combined Energy Layer\n",
        "        self.combined_layer = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # BiMMOE Experts\n",
        "        self.experts = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(hidden_dim * 3 // 2, hidden_dim // 4),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim // 4, hidden_dim // 8),\n",
        "                nn.ReLU()\n",
        "            ) for _ in range(3)\n",
        "        ])\n",
        "        # Gate Layer\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 3 // 2, len(self.experts)),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "        # Coupling Layer\n",
        "        self.coupling_layer = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 3 // 2 + hidden_dim // 8, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # Output Layer\n",
        "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        # Quantum Tunneling Dynamics\n",
        "        quantum_output = self.quantum_layer(x) * QUANTUM_RATIO\n",
        "        # Gravitational Funneling Dynamics\n",
        "        gravitational_output = self.gravitational_layer(x) * (GRAVITATIONAL_RATIO * (1 - BETA))\n",
        "        gravitational_output = gravitational_output - GAMMA * gravitational_output  # Added gamma for energy damping\n",
        "        # Combined Layer\n",
        "        combined_output = self.combined_layer(x) * COMBINED_RATIO\n",
        "        combined_output = LAMBDA * quantum_output + (1 - LAMBDA) * gravitational_output\n",
        "        # Add noise, CMB, and expansion\n",
        "        noise = torch.normal(mean=0, std=self.noise_std, size=combined_output.size()).to(x.device)\n",
        "        expansion_factor = 1 + EXPANSION_RATE * t\n",
        "        combined_output = (combined_output + noise + CMB_NOISE) * expansion_factor\n",
        "\n",
        "        # Full Combined Layers\n",
        "        full_combined = torch.cat((quantum_output, gravitational_output, combined_output), dim=1)\n",
        "        gate_weights = self.gate(full_combined)\n",
        "        expert_outputs = torch.stack([expert(full_combined) for expert in self.experts], dim=1)\n",
        "        weighted_output = torch.sum(gate_weights.unsqueeze(-1) * expert_outputs, dim=1)\n",
        "        concatenated_output = torch.cat((full_combined, weighted_output), dim=1)\n",
        "\n",
        "        # Coupling and Output\n",
        "        coupled_output = self.coupling_layer(concatenated_output)\n",
        "        output = self.output_layer(coupled_output)\n",
        "        return torch.softmax(output, dim=1)\n",
        "\n",
        "# Training with Validation F1 and Dynamic Learning Rate\n",
        "def train_model_with_f1(\n",
        "    model, criterion, optimizer, scheduler, X_train, y_train, X_val, y_val, num_epochs=50\n",
        "):\n",
        "    best_val_f1 = 0.0  # Track the best validation F1 score\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(X_train, torch.tensor(epoch, dtype=torch.float32))\n",
        "        loss = criterion(outputs, y_train)\n",
        "        loss.backward()\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(X_val, torch.tensor(epoch, dtype=torch.float32))\n",
        "            val_loss = criterion(val_outputs, y_val)\n",
        "            val_preds = torch.argmax(val_outputs, axis=1)\n",
        "            val_true = torch.argmax(y_val, axis=1)\n",
        "            val_f1 = f1_score(val_true.cpu().numpy(), val_preds.cpu().numpy(), average=\"macro\")\n",
        "\n",
        "        # Update best validation F1 score\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch+1}/{num_epochs}, \"\n",
        "            f\"Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, \"\n",
        "            f\"F1 Score: {val_f1:.4f}, Best Val F1: {best_val_f1:.4f}, \"\n",
        "            f\"Learning Rate: {scheduler.get_last_lr()[0]:.6f}\"\n",
        "        )\n",
        "\n",
        "# Generate Synthetic Data\n",
        "def generate_synthetic_data(num_samples=1000):\n",
        "    X = np.random.rand(num_samples, 1).astype(np.float32)\n",
        "    y = np.zeros((num_samples, 3), dtype=np.float32)\n",
        "    y[:, 0] = 0.68 * (0.95 + 0.1 * np.random.rand(num_samples))  # Quantum tunneling\n",
        "    y[:, 1] = 0.27 * (0.95 + 0.1 * np.random.rand(num_samples))  # Gravitational funneling\n",
        "    y[:, 2] = 1 - y[:, 0] - y[:, 1]  # Combined dynamics\n",
        "    return X, y\n",
        "\n",
        "# Prepare Data\n",
        "X, y = generate_synthetic_data()\n",
        "X_train, X_val = torch.tensor(X[:800]), torch.tensor(X[800:])\n",
        "y_train, y_val = torch.tensor(y[:800]), torch.tensor(y[800:])\n",
        "\n",
        "# Initialize Model\n",
        "input_dim = 1\n",
        "hidden_dim = 64\n",
        "output_dim = 3\n",
        "model = BiMMOEQDT_UniversalExpansion(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# Loss, Optimizer, and Scheduler\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=0.0001)\n",
        "\n",
        "# Train Model\n",
        "train_model_with_f1(model, criterion, optimizer, scheduler, X_train, y_train, X_val, y_val)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1E22B8tcxMeZ",
        "outputId": "16b39694-2a4b-4751-f565-5d7ededf955f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Train Loss: 0.0770, Val Loss: 0.0750, F1 Score: 0.0000, Best Val F1: 0.0000, Learning Rate: 0.000978\n",
            "Epoch 2/50, Train Loss: 0.0752, Val Loss: 0.0731, F1 Score: 0.0000, Best Val F1: 0.0000, Learning Rate: 0.000914\n",
            "Epoch 3/50, Train Loss: 0.0733, Val Loss: 0.0712, F1 Score: 0.0000, Best Val F1: 0.0000, Learning Rate: 0.000815\n",
            "Epoch 4/50, Train Loss: 0.0714, Val Loss: 0.0693, F1 Score: 0.0033, Best Val F1: 0.0033, Learning Rate: 0.000689\n",
            "Epoch 5/50, Train Loss: 0.0695, Val Loss: 0.0677, F1 Score: 0.1355, Best Val F1: 0.1355, Learning Rate: 0.000550\n",
            "Epoch 6/50, Train Loss: 0.0678, Val Loss: 0.0663, F1 Score: 0.3174, Best Val F1: 0.3174, Learning Rate: 0.000411\n",
            "Epoch 7/50, Train Loss: 0.0663, Val Loss: 0.0651, F1 Score: 0.4030, Best Val F1: 0.4030, Learning Rate: 0.000285\n",
            "Epoch 8/50, Train Loss: 0.0652, Val Loss: 0.0642, F1 Score: 0.4444, Best Val F1: 0.4444, Learning Rate: 0.000186\n",
            "Epoch 9/50, Train Loss: 0.0643, Val Loss: 0.0637, F1 Score: 0.4695, Best Val F1: 0.4695, Learning Rate: 0.000122\n",
            "Epoch 10/50, Train Loss: 0.0637, Val Loss: 0.0633, F1 Score: 0.4805, Best Val F1: 0.4805, Learning Rate: 0.000100\n",
            "Epoch 11/50, Train Loss: 0.0633, Val Loss: 0.0629, F1 Score: 0.4764, Best Val F1: 0.4805, Learning Rate: 0.000122\n",
            "Epoch 12/50, Train Loss: 0.0629, Val Loss: 0.0625, F1 Score: 0.4937, Best Val F1: 0.4937, Learning Rate: 0.000186\n",
            "Epoch 13/50, Train Loss: 0.0625, Val Loss: 0.0618, F1 Score: 0.4924, Best Val F1: 0.4937, Learning Rate: 0.000285\n",
            "Epoch 14/50, Train Loss: 0.0617, Val Loss: 0.0610, F1 Score: 0.4975, Best Val F1: 0.4975, Learning Rate: 0.000411\n",
            "Epoch 15/50, Train Loss: 0.0608, Val Loss: 0.0594, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000550\n",
            "Epoch 16/50, Train Loss: 0.0594, Val Loss: 0.0575, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000689\n",
            "Epoch 17/50, Train Loss: 0.0574, Val Loss: 0.0549, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000815\n",
            "Epoch 18/50, Train Loss: 0.0549, Val Loss: 0.0518, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000914\n",
            "Epoch 19/50, Train Loss: 0.0517, Val Loss: 0.0479, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000978\n",
            "Epoch 20/50, Train Loss: 0.0479, Val Loss: 0.0437, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.001000\n",
            "Epoch 21/50, Train Loss: 0.0437, Val Loss: 0.0393, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000978\n",
            "Epoch 22/50, Train Loss: 0.0392, Val Loss: 0.0347, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000914\n",
            "Epoch 23/50, Train Loss: 0.0346, Val Loss: 0.0304, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000815\n",
            "Epoch 24/50, Train Loss: 0.0301, Val Loss: 0.0263, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000689\n",
            "Epoch 25/50, Train Loss: 0.0261, Val Loss: 0.0229, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000550\n",
            "Epoch 26/50, Train Loss: 0.0227, Val Loss: 0.0203, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000411\n",
            "Epoch 27/50, Train Loss: 0.0201, Val Loss: 0.0182, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000285\n",
            "Epoch 28/50, Train Loss: 0.0181, Val Loss: 0.0169, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000186\n",
            "Epoch 29/50, Train Loss: 0.0166, Val Loss: 0.0159, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000122\n",
            "Epoch 30/50, Train Loss: 0.0156, Val Loss: 0.0153, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000100\n",
            "Epoch 31/50, Train Loss: 0.0149, Val Loss: 0.0145, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000122\n",
            "Epoch 32/50, Train Loss: 0.0143, Val Loss: 0.0138, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000186\n",
            "Epoch 33/50, Train Loss: 0.0137, Val Loss: 0.0129, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000285\n",
            "Epoch 34/50, Train Loss: 0.0128, Val Loss: 0.0118, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000411\n",
            "Epoch 35/50, Train Loss: 0.0116, Val Loss: 0.0103, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000550\n",
            "Epoch 36/50, Train Loss: 0.0101, Val Loss: 0.0085, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000689\n",
            "Epoch 37/50, Train Loss: 0.0084, Val Loss: 0.0068, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000815\n",
            "Epoch 38/50, Train Loss: 0.0067, Val Loss: 0.0053, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000914\n",
            "Epoch 39/50, Train Loss: 0.0052, Val Loss: 0.0044, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000978\n",
            "Epoch 40/50, Train Loss: 0.0043, Val Loss: 0.0041, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.001000\n",
            "Epoch 41/50, Train Loss: 0.0042, Val Loss: 0.0045, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000978\n",
            "Epoch 42/50, Train Loss: 0.0046, Val Loss: 0.0052, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000914\n",
            "Epoch 43/50, Train Loss: 0.0054, Val Loss: 0.0058, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000815\n",
            "Epoch 44/50, Train Loss: 0.0060, Val Loss: 0.0062, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000689\n",
            "Epoch 45/50, Train Loss: 0.0065, Val Loss: 0.0063, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000550\n",
            "Epoch 46/50, Train Loss: 0.0066, Val Loss: 0.0063, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000411\n",
            "Epoch 47/50, Train Loss: 0.0066, Val Loss: 0.0063, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000285\n",
            "Epoch 48/50, Train Loss: 0.0065, Val Loss: 0.0062, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000186\n",
            "Epoch 49/50, Train Loss: 0.0064, Val Loss: 0.0061, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000122\n",
            "Epoch 50/50, Train Loss: 0.0063, Val Loss: 0.0061, F1 Score: 1.0000, Best Val F1: 1.0000, Learning Rate: 0.000100\n"
          ]
        }
      ]
    }
  ]
}
